# Data Engineer
#### Technical Skills: Python, SQL, Azure, GCP, Hadoop, Apache Spark, Apache Airflow, Elasticsearch

## Education
- M.Eng., ECE | Ontario Tech University (2022)								       		
- B.Tech., ECE  | SRM Institute of Science & Tech (2018)

## Work Experience
**Data Engineer @ Scotiabank (_June 2023 - Present_)**
- Implemented an in-house PySpark-based framework to consolidate data from various sources, utilizing Parquet for data ingestion and historical data conversion, resulting in a 30% reduction in storage costs within 9 months.
- Conducted strategic ETL pipeline data migration, enhancing performance by 20% through rigorous post-conversion testing and ad-hoc troubleshooting of data type mismatches and encoding issues, ensuring the efficient weekly transfer of ~4 TB of data.
- Maximized data integration accuracy by 95% through optimized marker files and COBOL copybooks, aligning data with evolving business requirements.
Collaborated with cross-functional teams to manage project life cycle using JIRA ensuring adherence to project timelines and reducing issue resolution time by 15% through JIRA.
- Produced comprehensive dataset documentation and metadata, facilitating ease of understanding and utilization by stakeholders, resulting in a 15% decrease in data-related errors and issues, amounting to annual savings of $40K in operational costs.
- Gathered and analysed business and functional requirements, translating them into robust, scalable solutions aligned with organizational data architecture.

**Systems Engineer @ Infosys(Client - Pfizer) (_August 2018 - June 2020_)** 
- Improved report generation time by 75%, from 4 days to <1 day by developing automated bi-weekly Power BI dashboards, allowing for faster data-driven decisions.
- Implemented data ingestion routines, encompassing both real-time and batch processes, adhering to best practices in data modelling. Achieved a reduction in data ingestion latency by 30% through optimization techniques.
- Engineered efficient SQL pipelines to analyse 1500 web application logs, resulting in a 12% reduction in recurring errors within 6 months for a leading US pharmaceutical company.
- Developed ETL processes leveraging Azure technologies and Big Data tools, resulting in a 40% decrease in data processing time, translating to annual savings of $80K in infrastructure costs.
- Optimized 20+ batch processes using Python scripts, resulting in an 8% increase in ticket volume and decreased maintenance requirements.
- Streamlined and shared 50+ data processes, reducing high-priority ticket resolution time by over 20% and optimizing access control.
- Produced comprehensive and user-friendly dataset documentation and metadata to facilitate seamless data management and understanding.

## PROJECTS
**Netflix Data Insights Platform on Azure (_Jan 2024 – Feb 2024_)**
- Constructed and deployed a data pipeline using Azure SQL Database and Data Factory, migrating the Netflix dataset from SQL Server to Azure Storage. Enhanced scalability and data processing using Azure technologies, enabling advanced analytical operations.
- Orchestrated data transformation and aggregation workflows using Azure Databricks and Spark SQL, culminating in the integration with Power BI for advanced data visualization and insights, facilitating data-driven decision-making processes.
**Sales Data Elevate: Cloud-Based ETL and Analytics Transformation (_Jan 2024 – Feb 2024_)**
- Engineered and launched a sophisticated data processing pipeline using Azure Data Lake Storage and Azure Data Factory to migrate Sales data into a cloud ecosystem. Enhanced data access and scalability with Azure, supporting complex analytical processes.
- Architected and executed advanced data transformation and enrichment strategies with Azure Databricks and SparkSQL, seamlessly integrating with Azure Synapse Analytics for comprehensive data analysis.
**ETL Pipeline for News Article Analysis (_Nov 2023 – Dec 2023_)**
- Designed a robust, fault tolerant ETL pipeline using Python, Apache Kafka, Hadoop, and Hive to ingest and analyse real-time news articles from the NEWS API, handling over 1000 requests daily efficiently.
- Improved the data streaming system with advanced Hive analytics on Google DataProc, enabling identification of trends like highest author count and most repeated keywords in the dataset.

## Contact Details
- [Linkedin](https://www.linkedin.com/in/vaibhavjain01/)
- Email - vaibhav.jain9607@gmail.com**
